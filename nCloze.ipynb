{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from string import punctuation\n",
    "import sys\n",
    "import math\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections as mc\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "from math import log,exp,sqrt\n",
    "\n",
    "# Set up logger\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "from typing import List, Dict, Tuple, Type, Union\n",
    "from torch import Tensor, device\n",
    "from torch.autograd import Variable\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, AutoConfig, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "from statistics import mean\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk import FreqDist\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "cos = T.nn.CosineSimilarity(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROPS_OF_TEXT_TO_READ_FROM = [0.5] # proportion of the text passage to not blank out\n",
    "NUM_OF_CHOICES = 4 # number of answer choices per fill-in\n",
    "DISTRACTOR_POOL = 32\n",
    "\n",
    "CONTEXTUAL_EMBEDDING_LAYERS = [4] # layers to sum contextual embeddings over\n",
    "SIM_ANNEAL_EMB_WEIGHT = 8#4 # how much to weigh embedding distance vs. word likelihoods (increasing this value decreases plausibility while increasing diversity)\n",
    "CORRECTNESS_SENT = 1 # how much to weight probability of tokens in sentence-context model\n",
    "PLAUSIBILITY_DISTR_TO_ANS = 8#.25\n",
    "\n",
    "MAX_QUESTIONS_BY_N = 16\n",
    "INTERVAL_BETWEEN_WORDS = 7\n",
    "EXTEND_SUBWORDS = True # attempt to grow show short, infrequent (or unrecognized) distractor tokens by adding suffixes until they become words or until a limit is reached.\n",
    "MAX_SUBWORDS = 3 # max number of suffixes to try adding before creating a valid word\n",
    "DISTRACTORS_FROM_TEXT = False\n",
    "EXTEND_BEAM_WIDTH = 5\n",
    "MIN_SENT_WORDS = 7\n",
    "\n",
    "DEBUG_OUTPUT = True\n",
    "\n",
    "CACHE_DIR = \n",
    "MODEL_TYPE = \"roberta-large\" # Masked LM huggingface model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintain reproducibility\n",
    "T.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Quality-Check Tools for Distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencies are used to decide if a distractor candidate might be a subword\n",
    "stemmer = PorterStemmer()\n",
    "freq = FreqDist(i.lower() for i in gutenberg.words())\n",
    "print(freq.most_common()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_unix = set(line.strip() for line in open('dict-unix.txt'))\n",
    "words_info = set(line.strip() for line in open('dict-info.txt'))\n",
    "words_small = words_unix.intersection(words_info)\n",
    "words_large = words_unix.union(words_info)\n",
    "f = open('profanity.json')\n",
    "profanity = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_TYPE, cache_dir=CACHE_DIR)\n",
    "model.cuda()\n",
    "toker = AutoTokenizer.from_pretrained(MODEL_TYPE, add_prefix_space=True)\n",
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', model_dir='/data/ondovbd/stanza_resources')\n",
    "\n",
    "nltk_sent_toker = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word(str):\n",
    "    '''Check if word exists in dictionary'''\n",
    "    splt = str.lower().split(\"'\")\n",
    "    if len(splt) > 2:\n",
    "        return False\n",
    "    elif len(splt) == 2:\n",
    "        return is_word(splt[0]) and (splt[1] in ['t','nt','s','ll'])\n",
    "    elif '-' in str:\n",
    "        for word in str.split('-'):\n",
    "            if not is_word(word):\n",
    "                return False\n",
    "        return True\n",
    "    else:\n",
    "        return str.lower() in words_unix or str.lower() in words_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_toker_vocab_dict = sorted(toker.vocab.items(), key=lambda x:x[1])\n",
    "suffix_mask = T.FloatTensor([1 if (('Ä ' != x[0][0]) and (re.match(\"^[A-Za-z0-9']*$\", x[0]) is not None)) else 0 for x in sorted_toker_vocab_dict]) # 1 means is-suffix and 0 mean not-suffix\n",
    "suffix_mask_inv = suffix_mask * -1 + 1\n",
    "word_mask = suffix_mask_inv*T.FloatTensor([1 if is_word(x[0][1:]) and x[0][1:].lower() not in profanity else 0 for x in sorted_toker_vocab_dict])\n",
    "suffix_mask=suffix_mask.cuda()\n",
    "suffix_mask_inv=suffix_mask_inv.cuda()\n",
    "word_mask = word_mask.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(snt_toks, tgt_toks, layers=None):\n",
    "    '''Embeds a group of subword tokens in place of a mask, using the entire\n",
    "    sentence for context. Returns the average of the target token embeddings,\n",
    "    which are summed over the hidden layers.\n",
    "\n",
    "    snt_toks: the tokenized sentence, including the mask token\n",
    "    tgt_toks: the tokens (subwords) to replace the mask token\n",
    "    layers (optional): which hidden layers to sum (list of indices)'''\n",
    "    mask_idx = snt_toks.index(toker.mask_token_id)\n",
    "    snt_toks = snt_toks.copy()\n",
    "\n",
    "    while mask_idx + len(tgt_toks)-1 >= 512:\n",
    "        # Shift text by 100 words\n",
    "        snt_toks = snt_toks[100:]\n",
    "        mask_idx -= 100\n",
    "    \n",
    "    snt_toks[mask_idx:mask_idx+1] = tgt_toks\n",
    "    snt_toks = snt_toks[:512]\n",
    "    with T.no_grad():\n",
    "        output = model(T.tensor([snt_toks]).cuda(), T.tensor([[1]*len(snt_toks)]).cuda(), output_hidden_states=True)\n",
    "    layers = CONTEXTUAL_EMBEDDING_LAYERS if layers is None else layers\n",
    "    output = T.stack([output.hidden_states[i] for i in layers]).sum(0).squeeze()\n",
    "    # Only select the tokens that constitute the requested word\n",
    "    return output[mask_idx:mask_idx+len(tgt_toks)].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend(toks_sent, toks_para, suff_ids, n_masks, ctx_words):\n",
    "    '''Get the most likely suffix for a subword in the mask position\n",
    "    \n",
    "    tokens: the tokens of the sentence with its one original mask token\n",
    "    sub_ids: the current list of subwords to replace the mask token with'''\n",
    "    sm_sent = get_softmax_logits(toks_sent, n_masks, suff_ids)\n",
    "    sm_para = get_softmax_logits(toks_para, n_masks, suff_ids)\n",
    "    \n",
    "    sm = T.exp((sm_sent[-1].log()+sm_para[-1].log())/2)\n",
    "    topk_pfx = T.topk(sm*suffix_mask_inv, EXTEND_BEAM_WIDTH)\n",
    "    best_ids = []\n",
    "    best_prob = 0\n",
    "    \n",
    "    for i in range(len(topk_pfx.indices)):\n",
    "        dec = toker.decode([topk_pfx.indices[i]]+suff_ids).strip()\n",
    "        if is_word(dec) or dec in ctx_words:\n",
    "            best_ids = [topk_pfx.indices[i]]\n",
    "            best_prob = topk_pfx.values[i]\n",
    "            break\n",
    "    \n",
    "    if n_masks > 1:\n",
    "        topk_sfx = T.topk(sm*suffix_mask, EXTEND_BEAM_WIDTH)\n",
    "        \n",
    "        for i in range(EXTEND_BEAM_WIDTH):\n",
    "            rec_suff_ids = [int(topk_sfx.indices[i])] + suff_ids\n",
    "            ids_sfx, prob_sfx = extend(toks_sent, toks_para, rec_suff_ids, n_masks-1, ctx_words)\n",
    "            \n",
    "            if prob_sfx > best_prob:\n",
    "                best_ids = ids_sfx + [int(topk_sfx.indices[i])]\n",
    "                best_prob = prob_sfx\n",
    "        \n",
    "    return best_ids, best_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy(ctx, scaled_dists, scaled_sims, choices, words, ans):\n",
    "    \n",
    "    #Calculate and add cosine similarity scores \n",
    "    '''Cost function to help choose best distractors'''\n",
    "    #e = [embs[i] for i in choices] #+ [sem_emb_ans]\n",
    "    #w = [words[i] for i in choices] #+ [ans]\n",
    "    \n",
    "    hm_sim = 0\n",
    "    e_ctx = 0\n",
    "    for i in choices:\n",
    "        hm_sim += 1./scaled_sims[i]\n",
    "        e_ctx += ctx[i]\n",
    "    \n",
    "    e_sim = float(len(choices))/hm_sim\n",
    "    \n",
    "    hm_emb = 0\n",
    "    count = 0\n",
    "    c = choices + [len(ctx)]\n",
    "    for i in range(len(c)):\n",
    "        for j in range(i):\n",
    "            d = scaled_dists['%s-%s'%(max(c[i],c[j]), min(c[i], c[j]))]\n",
    "            #print(c[i], c[j], d)\n",
    "            hm_emb += 1./d\n",
    "            count += 1\n",
    "    e_emb = float(count)/hm_emb\n",
    "    return float(e_emb), e_ctx, float(e_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anneal(probs_sent_context, probs_para_context, embs, emb_ans, words, k, ans):\n",
    "    '''find k distractor indices that are optimally high probability and distant\n",
    "    in embedding space'''\n",
    "#    probs_sent_context = T.as_tensor(probs_sent_context) / sum(probs_sent_context)\n",
    "    m = len(probs_sent_context)\n",
    "#    probs_para_context = T.as_tensor(probs_para_context) / sum(probs_para_context)\n",
    "    its = 1000\n",
    "    n = len(probs_para_context)\n",
    "    choices = list(range(k))\n",
    "    \n",
    "    dists = {}\n",
    "    embsa = embs + [emb_ans]\n",
    "    for i in range(len(embsa)):\n",
    "        for j in range(i):\n",
    "            dists['%s-%s'%(i,j)] = 1-cos(embsa[i], embsa[j]) # cosine \"distance\"\n",
    "            #print(words[i], words[j], 1-cos(embs[i], embs[j]))\n",
    "    \n",
    "    dist_min = T.min(T.tensor(list(dists.values())))\n",
    "    dist_max = T.max(T.tensor(list(dists.values())))\n",
    "    for key, dist in dists.items():\n",
    "        dists[key] = (dist - dist_min)/(dist_max-dist_min)\n",
    "    \n",
    "    sims = T.tensor([cos(emb_ans, emb) for emb in embs])\n",
    "    scaled_sims = (sims - T.min(sims))/(T.max(sims)-T.min(sims))\n",
    "    \n",
    "    ctx = T.tensor(probs_sent_context).log()-ALPHA*T.tensor(probs_para_context).log()\n",
    "    ctx = (ctx-T.min(ctx))/(T.max(ctx)-T.min(ctx))\n",
    "    \n",
    "    e_emb, e_ctx, e_sim = energy(ctx, dists, scaled_sims, choices, words, ans)\n",
    "    e = e_ctx + BETA * e_emb\n",
    "    #e = SIM_ANNEAL_EMB_WEIGHT * e_emb + e_prob\n",
    "    for i in range(its):\n",
    "        t = 1.-(i)/its\n",
    "        mut_idx = random.randrange(k) # which choice to mutate\n",
    "        orig = choices[mut_idx]\n",
    "        new = orig\n",
    "        while (new in choices): # mutate choice until not in current list\n",
    "            new = random.randrange(m)\n",
    "        choices[mut_idx] = new\n",
    "        e_emb, e_ctx, e_sim = energy(ctx, dists, scaled_sims, choices, words, ans)\n",
    "        e_new = e_ctx + BETA * e_emb\n",
    "        delta = e_new - e\n",
    "        exponent = delta/t\n",
    "        if exponent < -50:\n",
    "            exponent = -50 # avoid underflow\n",
    "        if delta > 0 or math.exp(exponent) > random.random():\n",
    "            e = e_new # accept new state\n",
    "        else:\n",
    "            choices[mut_idx] = orig\n",
    "    if DEBUG_OUTPUT:\n",
    "        print([words[j] for j in choices] + [ans], \"e: %f\"%(e))\n",
    "    return choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distractor Generator Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_softmax_logits(toks, n_masks = 1, sub_ids = []):\n",
    "    # Tokenize text - Keep length of inpts at or below 512 (including answer token length artifically added at end)\n",
    "    msk_idx = toks.index(toker.mask_token_id)\n",
    "    toks = toks.copy()\n",
    "    toks[msk_idx:msk_idx+1] = [toker.mask_token_id] * n_masks + sub_ids\n",
    "    \n",
    "    # If the masked_token is over 512 (excluding answer token length artifically added at end) tokens away\n",
    "    while msk_idx >= 512:\n",
    "        # Shift text by 100 words\n",
    "        toks = toks[100:]\n",
    "        msk_idx -= 100\n",
    "    toks = toks[:512]\n",
    "    # Find the predicted words for the fill-in-the-blank mask term based on sentence-context alone\n",
    "    with T.no_grad():\n",
    "        output = model(T.tensor([toks]).cuda(), T.tensor([[1]*len(toks)]).cuda())\n",
    "    sm = T.softmax(output.logits[0, msk_idx:msk_idx+n_masks, :], dim=1)\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=1e-10\n",
    "\n",
    "def candidates(text, answer):\n",
    "    '''Create list of unique distractors that does not include the actual answer'''\n",
    "    if DEBUG_OUTPUT:\n",
    "        print(text)\n",
    "    \n",
    "    # Get only sentence with blanked text to tokenize\n",
    "    doc = nlp(text)\n",
    "    #sents = [sentence.text for sentence in doc.sentences]\n",
    "    sents = nltk_sent_toker.tokenize(text)\n",
    "    msk_snt_idx = [i for i in range(len(sents)) if '<mask>' in sents[i]][0]\n",
    "    just_masked_sentence = sents[msk_snt_idx]\n",
    "    \n",
    "    prv_snts = sents[:msk_snt_idx]\n",
    "    nxt_snts = sents[msk_snt_idx+1:]\n",
    "    \n",
    "    if len(just_masked_sentence.split(' ')) < MIN_SENT_WORDS and len(prv_snts):\n",
    "        just_masked_sentence = ' '.join([prv_snts.pop(), just_masked_sentence])\n",
    "    \n",
    "    while len(just_masked_sentence.split(' ')) < MIN_SENT_WORDS and (len(prv_snts) or len(nxt_snts)):\n",
    "        if T.rand(1) < 0.5 and len(prv_snts):\n",
    "            just_masked_sentence = ' '.join([prv_snts.pop(), just_masked_sentence])\n",
    "        elif len(nxt_snts):\n",
    "            just_masked_sentence = ' '.join([just_masked_sentence, nxt_snts.pop(0)])\n",
    "    \n",
    "    ctx = just_masked_sentence\n",
    "    while len(ctx.split(' ')) < 3 * len(just_masked_sentence.split(' ')) and (len(prv_snts) or len(nxt_snts)):\n",
    "        if len(prv_snts):\n",
    "            ctx = ' '.join([prv_snts.pop(), ctx])\n",
    "        if len(nxt_snts):\n",
    "            ctx = ' '.join([ctx, nxt_snts.pop(0)])\n",
    "    \n",
    "#    just_masked_sentence = ' '.join([just_masked_sentence.replace('<mask>', 'banana'),\n",
    "#                                     just_masked_sentence.replace('<mask>', 'banana'),\n",
    "##                                     just_masked_sentence,\n",
    "  #                                   just_masked_sentence.replace('<mask>', 'banana'),\n",
    "   #                                  just_masked_sentence.replace('<mask>', 'banana')])\n",
    "    #just_masked_sentence = ' '.join([just_masked_sentence, just_masked_sentence, just_masked_sentence, just_masked_sentence, just_masked_sentence])\n",
    "    \n",
    "    tiled = just_masked_sentence\n",
    "    while len(tiled) < len(text):\n",
    "        tiled += ' ' + just_masked_sentence\n",
    "    just_masked_sentence = tiled\n",
    "    \n",
    "    if DEBUG_OUTPUT:\n",
    "        print(ctx)\n",
    "        print(just_masked_sentence)\n",
    "    toks_para = toker.encode(text)\n",
    "    toks_sent = toker.encode(just_masked_sentence)\n",
    "    # Get softmaxed logits from sentence alone and full-text\n",
    "#    sent_sm, sent_pos, sent_ids = get_span_logits(just_masked_sentence, answer)\n",
    "#    para_sm, para_pos, para_ids = get_span_logits(text, answer)\n",
    "    \n",
    "    sent_sms_all = []\n",
    "    para_sms_all = []\n",
    "    para_sms_right = []\n",
    "    \n",
    "    for i in range(MAX_SUBWORDS):\n",
    "        para_sms = get_softmax_logits(toks_para, i + 1)\n",
    "        para_sms_all.append(para_sms)\n",
    "        sent_sms = get_softmax_logits(toks_sent, i + 1)\n",
    "        sent_sms_all.append(sent_sms)\n",
    "        para_sms_right.append(T.exp((sent_sms[i].log()+para_sms[i].log())/2) * (suffix_mask_inv if i == 0 else suffix_mask))\n",
    "    \n",
    "    # Create 2 lists: (1) notes highest probability for each token across n-mask lists if token is suffix and (2) notes number of mask terms to add\n",
    "    para_sm_best, para_pos_best = T.max(T.vstack(para_sms_right), 0)\n",
    "    \n",
    "    distractors = []\n",
    "    stems = []\n",
    "    embs = []\n",
    "    sent_probs = []\n",
    "    para_probs = []\n",
    "    \n",
    "    ans_stem = stemmer.stem(answer.lower())\n",
    "    \n",
    "    emb_ans = get_emb(toks_para, toker(answer)['input_ids'][1:-1])\n",
    "    para_words = text.lower().split(' ')\n",
    "    blank_word_idx = [idx for idx, word in enumerate(para_words) if '<mask>' in word][0] # Need to remove punctuation\n",
    "    if (blank_word_idx - 1) < 0:\n",
    "        prev_word = 'beforeanytext'\n",
    "    else:\n",
    "        prev_word = para_words[blank_word_idx-1]\n",
    "    if (blank_word_idx + 1) >= len(para_words):\n",
    "        next_word = 'afteralltext'\n",
    "    else:\n",
    "        next_word = para_words[blank_word_idx+1]\n",
    "    \n",
    "    # Need to check if the token is outside of the tokenizer based on predictions being made at all\n",
    "    if len(para_sms_all[0]) > 0:\n",
    "        top_ctx = T.topk((sent_sms_all[0][0]*word_mask+e).log() - ALPHA * (para_sms_all[0][0]*word_mask+e).log(), len(para_sms_all[0][0]), dim=0)\n",
    "        para_top_ids = top_ctx.indices.tolist()\n",
    "        para_top_probs = top_ctx.values.tolist()\n",
    "        \n",
    "        for i, id in enumerate(para_top_ids):\n",
    "            \n",
    "            sub_ids = [int(id)] # cumulative list of subword token ids\n",
    "            dec = toker.decode(sub_ids).strip()\n",
    "            if DEBUG_OUTPUT:\n",
    "                print('Trying:', dec)\n",
    "            #print(para_pos[id])\n",
    "            #if para_pos_best[id] > 0:\n",
    "            #    continue\n",
    "            \n",
    "            if dec.isupper() != answer.isupper():\n",
    "                continue\n",
    "            \n",
    "            if EXTEND_SUBWORDS and para_pos_best[id] > 0:\n",
    "                if DEBUG_OUTPUT:\n",
    "                    print(\"Extending %s with %d masks...\"%(dec, para_pos_best[id]))\n",
    "                ext_ids, _ = extend(toks_sent, toks_para, [id], para_pos_best[id], para_words)\n",
    "                sub_ids = ext_ids + sub_ids\n",
    "                dec_ext = toker.decode(sub_ids).strip()\n",
    "                if DEBUG_OUTPUT:\n",
    "                    print(\"Extended %s to %s\"%(dec, dec_ext))\n",
    "                if is_word(dec_ext) or (dec_ext != '' and dec_ext in para_words):\n",
    "                    dec = dec_ext # choose new word\n",
    "                else:\n",
    "                    sub_ids = [int(id)] # reset\n",
    "            \n",
    "            if len(toker.decode(sub_ids).lower().strip()) < 2:\n",
    "                continue\n",
    "                \n",
    "            if dec[0].isupper() != answer[0].isupper():\n",
    "                continue\n",
    "            \n",
    "            # Only add distractor if it does not contain punctuation\n",
    "            #if any(p in dec for p in punctuation):\n",
    "            #    pass\n",
    "                #continue\n",
    "            \n",
    "            if dec.lower() in profanity:\n",
    "                continue\n",
    "            \n",
    "            # make sure is a word, either in dict or somewhere else in text\n",
    "            if not is_word(dec) and dec.lower() not in para_words:\n",
    "                continue\n",
    "\n",
    "            # make sure is not the same as an adjacent word\n",
    "            if dec.lower() == prev_word or dec.lower() == next_word:\n",
    "                continue\n",
    "            \n",
    "            # Don't add the distractor if stem matches another\n",
    "            stem = stemmer.stem(dec).lower()\n",
    "            if stem in stems or stem == ans_stem:\n",
    "                continue\n",
    "\n",
    "            # Only add distractor if it does not contain a number\n",
    "            if any(char.isdigit() for char in toker.decode([id])):\n",
    "                continue\n",
    "\n",
    "            # Only add distractor if the distractor exists in the text already\n",
    "            if DISTRACTORS_FROM_TEXT and dec.lower() not in para_words:\n",
    "                continue\n",
    "            \n",
    "            #if answer[0].isupper():\n",
    "            #    dec = dec.capitalize()\n",
    "            \n",
    "            # PASSED ALL TESTS; finally add distractor and computations\n",
    "            distractors.append(dec)\n",
    "            stems.append(stem)\n",
    "            sent_logprob = 0\n",
    "            para_logprob = 0\n",
    "            nsubs = len(sub_ids)\n",
    "            for j in range(nsubs):\n",
    "                sub_id = sub_ids[j]\n",
    "                sent_logprob_j = log(sent_sms_all[nsubs-1][j][sub_id])\n",
    "                para_logprob_j = log(para_sms_all[nsubs-1][j][sub_id])\n",
    "                #if j == 0 or sent_logprob_j > sent_logprob:\n",
    "                #    sent_logprob = sent_logprob_j\n",
    "                #if j == 0 or para_logprob_j > para_logprob:\n",
    "                #    para_logprob = para_logprob_j\n",
    "                sent_logprob += sent_logprob_j\n",
    "                para_logprob += para_logprob_j\n",
    "            sent_logprob /= nsubs\n",
    "            para_logprob /= nsubs\n",
    "            if DEBUG_OUTPUT:\n",
    "                print(\"%s (p_sent=%f, p_para=%f)\"%(dec,sent_logprob,para_logprob))\n",
    "            sent_probs.append(exp(sent_logprob))\n",
    "            para_probs.append(exp(para_logprob))\n",
    "#            sent_probs.append(sent_sms_all[nsubs-1][nsubs-1][sub_id])\n",
    "#            para_probs.append(para_sms_all[nsubs-1][nsubs-1][sub_id])\n",
    "            embs.append(get_emb(toks_para, sub_ids))\n",
    "\n",
    "            if len(distractors) >= DISTRACTOR_POOL:\n",
    "                break\n",
    "    if DEBUG_OUTPUT:\n",
    "        print('Corresponding Text: ', text)\n",
    "        print('Correct Answer: ', answer)\n",
    "        print('Distractors created before annealing: ', distractors)\n",
    "    #indices = anneal(sent_probs, para_probs, embs, emb_ans, number_of_distractors, distractors, answer)\n",
    "    #distractors = [distractors[i] for i in indices]\n",
    "    #distractors += [''] * (number_of_distractors - len(distractors))\n",
    "        \n",
    "    return sent_probs, para_probs, embs, emb_ans, distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distractors(text, answer):\n",
    "    sent_probs, para_probs, embs, emb_ans, distractors = candidates(text, answer)\n",
    "    #print(distractors)\n",
    "    indices = anneal(sent_probs, para_probs, embs, emb_ans, distractors, 3, answer)\n",
    "    return [distractors[x] for x in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_positions(text):\n",
    "    sents = nltk_sent_toker.tokenize(text)\n",
    "    msk_snt_idx = [i for i in range(len(sents)) if '<mask>' in sents[i]][0]\n",
    "    just_masked_sentence = sents[msk_snt_idx]\n",
    "    \n",
    "    prv_snts = sents[:msk_snt_idx]\n",
    "    nxt_snts = sents[msk_snt_idx+1:]\n",
    "    \n",
    "    i = 0\n",
    "    while len(just_masked_sentence.split(' ')) < MIN_SENT_WORDS and (len(prv_snts) or len(nxt_snts)):\n",
    "        if i % 2 == 0 and len(prv_snts):\n",
    "            just_masked_sentence = ' '.join([prv_snts.pop(), just_masked_sentence])\n",
    "        elif len(nxt_snts):\n",
    "            just_masked_sentence = ' '.join([just_masked_sentence, nxt_snts.pop(0)])\n",
    "        i += 1\n",
    "    \n",
    "#    tiled = just_masked_sentence\n",
    "#    while len(tiled) < len(text):\n",
    "#        tiled += ' ' + just_masked_sentence\n",
    "#    just_masked_sentence = tiled\n",
    "    \n",
    "    if DEBUG_OUTPUT:\n",
    "        print(just_masked_sentence)\n",
    "    toks_para = toker.encode(text)\n",
    "    toks_sent = toker.encode(just_masked_sentence)\n",
    "    \n",
    "    para_sms = get_softmax_logits(toks_para, 1)[0]\n",
    "    sent_sms = get_softmax_logits(toks_sent, 1)[0]\n",
    "    ctx = (sent_sms*word_mask+e).log()-ALPHA*(para_sms*word_mask+e).log()\n",
    "    tk = T.topk(ctx, DISTRACTOR_POOL, dim=0)\n",
    "    top_ids = tk.indices.tolist()\n",
    "    top_probs = tk.values\n",
    "    \n",
    "    #inc = T.index_select(sent_sms, 0, tk.indices) - ALPHA*T.index_select(para_sms, 0, tk.indices)\n",
    "    #for i, idx in enumerate(top_ids):\n",
    "        #print(idx, sorted_toker_vocab_dict[idx], inc[i])\n",
    "    return T.sum(top_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(word, cdgp = False):\n",
    "    strp = word.strip(punctuation)\n",
    "    return word.replace(strp, '[MASK]' if cdgp else '<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_answer(distractors, answer):\n",
    "    idx = random.randint(0,3)\n",
    "    distractors.insert(idx, answer)\n",
    "    return distractors, idx2ans[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "DEBUG_OUTPUT=False\n",
    "DISTRACTOR_POOL = 128\n",
    "\n",
    "def choose_and_blank(text, count):\n",
    "    words = text.split()\n",
    "    scores = []\n",
    "    for i, word in enumerate(words):\n",
    "        masked = mask(words[i])\n",
    "        t = ' '.join(words[:i]+[masked]+words[i+1:])\n",
    "        scores.append(float(score_positions(t)))\n",
    "    print(scores)\n",
    "    blanks = []\n",
    "    answers = set()\n",
    "    while(len(blanks) < count):\n",
    "        tk = T.topk(T.tensor(scores), 1)\n",
    "        idx = tk.indices[0]\n",
    "        strp = words[idx].strip(punctuation)\n",
    "        if words[idx] == 'a(n' or words[idx][0].isupper() or any(char.isdigit() for char in strp) or words[idx] in answers or words[idx].lower() in stop_words:\n",
    "            scores[idx] = 0\n",
    "        else:\n",
    "            blanks.append(idx)\n",
    "            answers.add(strp)\n",
    "            for i in range(max(idx-4,0),min(idx+5,len(scores))):\n",
    "                scores[i] = 0\n",
    "    dists = []\n",
    "    answers = []\n",
    "    for blank in sorted(blanks):\n",
    "        masked = mask(words[blank])\n",
    "        t = ' '.join(words[:blank]+[masked]+words[blank+1:])\n",
    "        strp = words[blank].strip(punctuation)\n",
    "        ds = create_distractors(t, strp)\n",
    "        ds, letter = insert_answer(ds, strp)\n",
    "        dists.append(ds)\n",
    "        answers.append(letter)\n",
    "        print(ds, letter)\n",
    "    for blank in sorted(blanks):\n",
    "        masked = mask(words[blank])\n",
    "        words[blank] = masked.replace('<mask>', '_')\n",
    "    return ' '.join(words), dists, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "DEBUG_OUTPUT=False\n",
    "DISTRACTOR_POOL = 128\n",
    "    \n",
    "def choose_blanks(text, count):\n",
    "    words = text.split()\n",
    "    scores = []\n",
    "    for i, word in enumerate(words):\n",
    "        masked = mask(words[i])\n",
    "        t = ' '.join(words[:i]+[masked]+words[i+1:])\n",
    "        sents = nltk_sent_toker.tokenize(t)\n",
    "        scores.append(float(score_positions(t)))\n",
    "    scores = (T.tensor(scores)/1000).sigmoid()\n",
    "    blanks = []\n",
    "    blank_scores = []\n",
    "    answers = set()\n",
    "    c = 0\n",
    "    while(len(blanks) < count):\n",
    "        tk = T.topk(scores, 1)\n",
    "        idx = tk.indices[0]\n",
    "        strp = words[idx].strip(punctuation)\n",
    "        if tk.values[0] > 0 and (\n",
    "            len(strp) == 0 or\n",
    "            strp == 'a(n' or # no a(n)\n",
    "            strp[0].isupper() or # no Capitalized words\n",
    "            any(char.isdigit() for char in strp) or # no digits\n",
    "            any(char in punctuation.replace(\"'\", '').replace('-', '') for char in strp) or # no internal punctuation\n",
    "            strp in answers or # no repeats\n",
    "            strp.lower() in stop_words): # no stopwords\n",
    "            scores[idx] = 0\n",
    "        else:\n",
    "            blanks.append(idx)\n",
    "            blank_scores.append(scores[idx])\n",
    "            answers.add(strp)\n",
    "            for i in range(max(idx-MIN_DIST+1,0),min(idx+MIN_DIST,len(scores))):\n",
    "                scores[i] = 0\n",
    "    print(len(blanks), list(zip(blanks, blank_scores)))\n",
    "    return sorted(blanks)\n",
    "\n",
    "def create_distractors_for_blanks(text, blanks, cdgp=False):\n",
    "    words = text.split()\n",
    "    dists = []\n",
    "    answers = []\n",
    "    for blank in blanks:\n",
    "        masked = mask(words[blank], cdgp)\n",
    "        t = ' '.join(words[:blank]+[masked]+words[blank+1:])\n",
    "        strp = words[blank].strip(punctuation)\n",
    "        #print(t, strp)\n",
    "        ds = create_distractors_cdgp(t, strp) if cdgp else create_distractors(t, strp)\n",
    "        ds, letter = insert_answer(ds, strp)\n",
    "        dists.append(ds)\n",
    "        answers.append(letter)\n",
    "        #print(blank, ds, letter)\n",
    "    for blank in sorted(blanks):\n",
    "        masked = mask(words[blank])\n",
    "        words[blank] = masked.replace('[MASK]' if cdgp else '<mask>', '_')\n",
    "    return ' '.join(words), dists, answers\n",
    "\n",
    "def choose_and_blank(text, count):\n",
    "    blanks = choose_blanks(text, count)\n",
    "    return create_distractors_for_blanks(text, blanks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"A sad little boy was in search of happiness and wanted to meet God. On his way, he saw an elderly woman sitting in a park watching some birds. The boy sat down next to her. He opened his bag to take a drink. He noticed that lady looked hungry, so he offered her a piece of cake. She accepted and smiled at him. Her smile was so wonderful that he wanted to see it again. Then he offered her a can of coke. Once again she smiled at him. The boy was pleased! They stayed there all afternoon, eating and drinking without saying a word. As it began to grow dark, the boy got up to leave, but before he had gone no more than a few steps, he turned around, ran back to the old woman and gave her a big hug. She gave him her biggest smile. When the boy arrived home, his mother was surprised by the look of joy on his face. She asked, \"What has made you so happy today?\" He replied, \" I had lunch with God. She's got the most beautiful smile in the world!\" And when the old woman returned to her home, she told her son that she had lunch with God. Too often we overlook the power of a touch, a smile, a kind word, a listening ear or the smallest act of caring. However, all of these have the possible power to turn a life around.\"\"\"\n",
    "choose_and_blank(text, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emb(probs, embs, emb_ans, words, ans, ids):\n",
    "    \n",
    "    words = [ans]+words\n",
    "    probs = [np.max(probs)]+probs\n",
    "    embs = T.stack([emb_ans]+embs).cpu().numpy()\n",
    "    #tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    #new_values = tsne_model.fit_transform(embs)\n",
    "    pca = PCA(n_components=2)\n",
    "    new_values = pca.fit(embs).transform(embs)\n",
    "    \n",
    "    ofst = np.min(probs)\n",
    "    rnge = np.max(probs)-ofst\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "    \n",
    "    plt.figure(figsize=(24, 16)) \n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        #plt.scatter(x[i],y[i], c='white')\n",
    "\n",
    "        idx = i-1\n",
    "        scale = 0.9*(probs[idx]-ofst)/rnge\n",
    "        #print(scale)\n",
    "        plt.text(x[i], y[i], words[i], ha='center', va='center', size='large', c='magenta' if i == 0 else 'black', alpha=scale if i > 0 else 1, bbox=dict(edgecolor='magenta' if (i == 0) else 'black',facecolor='white', alpha=0.5) if (i == 0) else None)\n",
    "#        plt.text(x[i], y[i], words[i], alpha=-np.log(probs[i])/11)\n",
    "\n",
    "        plt.scatter(x[i],y[i],c='white',s=500)#,alpha=1-np.log(probs[i])*0.75)#,c=probs[i])\n",
    "    \n",
    "    ids = [0]+[x+1 for x in ids]\n",
    "    print(ids)\n",
    "    n=len(ids)\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            x1 = []\n",
    "            x2 = []\n",
    "            y1 = []\n",
    "            y2 = []\n",
    "            id_i = ids[i]\n",
    "            id_j = ids[j]\n",
    "            #print(i, j, i%n, j%n, id_i, id_j)\n",
    "            data.append((x[id_i], x[id_j]))\n",
    "            data.append((y[id_i], y[id_j]))\n",
    "    #plt.axis('off')\n",
    "    plt.plot(*data, alpha=0.25 ,color = 'black')\n",
    "    \n",
    "    plt.savefig(\"myImagePDF.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test pool generation\n",
    "DEBUG_OUTPUT = True\n",
    "MAX_SUBWORDS = 3\n",
    "text = \"\"\"The Healing Jim and his wife, Connie, were shocked by the loss of their four-month-old son--Joshua, whose life was taken by SIDS--sudden infant death syndrome. Thirty hours ago Jim drove to the baby-sitter's home to pick up Joshua. It was a routine trip, like the one he made five days every week. He arrived, and little Joshua could not be awakened from his nap. The next few hours were a time of life and death: the racing ambulance, swift-moving doctors and nurses....but 12 hours later, at children's Hospital, though the doctors had exhausted all  attempts, little Joshua was gone. Yes, they wanted all of Joshua's usable organs to be donated. That was not a difficult decision for Jim and Connie, a loving and giving couple. The next morning dawned and many things had to be arranged. Telephone calls and funeral plans. At one point Jim realized he needed a haircut. When Jim settled into the chair at the barber's, he began to reflect on the past hours, trying to make some sense of it all. Why had Joshua, their first-born, the child they had waited so long for, been taken so soon....he had barely begun his life....The question kept coming, and the pain in Jim's heart just enveloped him. While talking with the barber, Jim mentioned the organ donations, looking at his watch: \"They are transplanting one of his heart valves right now.\" The <mask> stopped and stood motionless. Finally she spoke, but it was only a whisper. \"You're not going to believe this....but about an hour ago the customer sitting in this chair wanted me to hurry so she could get to Children's Hospital. She left here so full of joy....her prayers had been answered. Today her baby granddaughter is receiving a desperately needed transplant--a heart transplant.\" Jim's healing began.\"\"\"\n",
    "answer = 'hairdresser'\n",
    "\n",
    "#text = \"\"\"Most animals, including snakes and fish, yawn, but it is only contagious in humans and chimps and, according to a recent study, dogs. The researchers, from the University of London's Birbeck College, put 29 <mask> in a room with a yawning man and found that 21, or 72%, also started to yawn. They said the skill may allow the pet to build stronger bonds with their owners.\"\"\"\n",
    "#answer = 'dogs'\n",
    "\n",
    "#text = \"\"\"Mr. Frieden had this to say: \"We won't be able to check travelers for fever when they leave or when they arrive. We won't be able, as we do presently, to take a detailed history to see if they were <mask> when they arrive. When they arrive, we wouldn't be able to impose quarantine as we now can if they have high-risk contact.\" \"\"\"\n",
    "#answer = 'exposed'\n",
    "\n",
    "#text = \"\"\"It is a time-proven fact that smile is a language. It is a universal language understood by the people of every nation, and the commonest way to show our good will perfectly without saying anything. One day I was shopping in a small town in California. It was my misfortune to be served by a clerk who seemed most unfriendly and not at all concerned about my intended purchase. I bought nothing, and walked angrily out of the store. My anger grew with each step. Outside, standing at the corner, was a young man in his early twenties. His expressive eyes met and held mine, and in the next instant a beautiful, amazing smile covered his face. I gave in immediately. The power of that shining smile drove away all my anger, and I found the muscles in my own face happily responding. \"Beautiful day, isn't it?\" I said. Then, suddenly something inside me sent me turning back. \"I really owe you a debt,â I said softly. His smile deepened, but he made no attempt to answer. A Mexican woman nearby stepped forward and said, \"Carlos can't speak English,\" she volunteered. \"Shall I tell him something?\" At that moment I felt changed. Carlos' smile had made a big person of me. \"Yes,\" my <mask> was enthusiastic and sincere. \"Tell him 'Thank you!'\" \"Thank you?\" The woman seemed slightly puzzled. \"Just tell him that,\" I insisted. \"Surely, he'll understand.\" What a smile! Although I have never seen that young man again, I'll never forget the lesson he taught me. From then on, I became smile-conscious. I practice it diligently, anywhere and everywhere, with everybody. This action on my part would always draw a good-natured smile in return.\"\"\"\n",
    "#answer = 'reply'\n",
    "\n",
    "#text = \"\"\"Have you ever heard the story of the four-minute mile? For years people believed that it is impossible for a human being to run a mile in less than four minutes until Roger Banister proved it wrong in 1954. Within one year, 37 runners broke the belief barrier. And the year after that, 300 other runners did the same thing. What happens if you put an animal in a pond? Any animal, big or small, will swim its way through. What happens when someone, who does not know how to swim, falls in deep waters? You <mask>. If an animal who has not learned swimming could escape by swimming, why not you? Because you believe you will drown while the animal does not. These cases show the power of beliefs. There is no other more powerful force in directing human behavior than belief. Our beliefs have the power to create and to destroy. In a way it is our beliefs that determine how much we'll be able to realize our potential. So pay attention to some of your beliefs. Do you believe you are weak in mathematics? Do you believe that other people dislike you? Do you believe life is full of problems? Belief is not mysterious, however. It's nothing but the generalization of a past incident. As a kid, if a dog bit you, you believed all dogs to be dangerous. To change certain behavior, identify the beliefs associated with it. Change those beliefs and a new pattern is automatically created.\"\"\"\n",
    "#answer = 'drown'\n",
    "\n",
    "#text = \"\"\"Two months ago, there was a serious earthquake in my country. Many people were hurt and lost their homes during the disaster. In my school, we decided to organize a 5-kilometer run to collect money for the people. Students signed up for the run and asked their relatives and neighbors to support them. These people agreed to give some money--50 cents or a dollar, for example, for every kilometer that the students completed. Joe was my classmate. He was the heaviest student in my class because he seldom did any exercise and he ate plenty of junk food. He never walked to school. He always took a bus. When he was asked <mask> he was going to take part in the run, he said he would think about it. A few students laughed and I think Joe felt embarrassed. I felt a bit sorry for him. The next day, as I was riding to school, I saw Joe walking in the street. I stopped and asked him why he was walking. He said he was training to take part in the run. \"Good for you, Joe!\" I said. Later, I told my friends about Joe. Most of them just laughed and said that they didn't think he would complete the run. I wasn't sure, but I decided that I would help Joe. So, for the two weeks before the run, I was his trainer. He walked to school for a few days . Then he started to run slowly. On the day of the run, Joe lined up with the other students. The race began and soon Joe was left behind. Well, it took him hours to finish and he didn't expect to catch up with the other students, but he tried to do his best. And finally he completed the 5-kilometer run. Everyone was very happy and said, \"Well done, Joe!\" What is more, Joe collected more money than any other student! What a good lesson we should learn from.\"\"\"\n",
    "#answer = 'if'\n",
    "\n",
    "#text = \"\"\"Wishing to encourage her young son's progress on the piano, a mother took the small boy to a Paderewski concert. After they were seated, the mother saw a friend in the audience and walked down the aisle to greet her. Seizing the opportunity to explore the wonders of the concert hall, the little boy rose and eventually explored his way through a door <mask> \"NO ADMITTANCE.\" When the house's lights dimmed and the concert was about to begin, the mother returned to her seat and discovered that her son was missing. Suddenly, the curtains parted and spotlights focused on the impressive Steinway on stage. In horror, the mother saw her little boy sitting at the keyboard, innocently picking out \"Twinkle, Twinkle Little Star.\" At that moment, the great piano master made his entrance, quickly moved to the piano, and whispered in the boy's ear, \"Don't quit.\" \"Keep playing.\" Then leaning over, Paderewski reached down with his left hand and began filling in a bass part. Soon his right arm reached around to the other side of the child and he added a running obbligato. Together, the old master and the young novice transformed a frightening situation into a wonderfully creative experience. The audience was mesmerized. That's the way it is with God. What we can finish on our own is hardly noteworthy. We try our best, but the results aren't exactly flowing music. But with the hand of the Master, our life's work truly can be beautiful.\"\"\"\n",
    "#answer = 'marked'\n",
    "\n",
    "#text = \"\"\"Eating chocolate is a great thing for most of us kids. If we want to eat it, we should pay for it. But there is a job that needs someone to taste chocolate every day. Isn't it wonderful? Laura Fagan is a 29-year-old British girl. She tastes chocolate, desserts and cakes every day and she can be paid well for this! Every day Laura Fagan tries different kinds of desserts for her supermarket, Teseo. She needs to try as many as 20 desserts a day. Also, she always travels for work. She needs to travel to different cities to try different desserts. Although the job seems to be great, it is hard. Usually, Laura begins trying desserts as early as 8 a.m., and is still trying new desserts at 6 p.m. before she goes back home. Fortunately, the job hasn't made her too fat. She only worries about her teeth. \"Of course I was afraid of becoming fat when I started the job, so I try to <mask> as often as possible in the gym. The main problem is my teeth. I don't think my dentists would be happy if I told them what I do, so I try to brush my teeth as often as possible.\" Laura loves her job although it is hard. \"I can learn about new trends in food,\" she said.\"\"\"\n",
    "#answer = 'exercise'\n",
    "\n",
    "#text=\"\"\"What kind of homes will we live in in the future? Nobody can be sure, but scientists are working out new ideas now. Some scientists are thinking about building whole cities under huge glass domes. Of course advanced heating and cooling systems will be necessary to control the weather in the domes. Therefore, there will never be any rain or snow, and the temperature will always be comfortable. Perhaps everyone will live in vertical cities -- high rises that are so large that they can contain all the necessities of life. Since vertical cities will use less land than flat cities, and provide homes for more people, they will be practical for small countries that have a large population. <mask> idea that will be helpful to small countries is the floating city. Monaco has already built homes, stores, and offices on the water of the Mediterranean Sea. There are some people who think that we will go back to living in caves. But the caves of the future will be very different from the caves of the Stone Age. Farms and parks will be on the land over the cave city. When people want to go to the country or to a park, a short ride in a lift will take them there.\"\"\"\n",
    "#answer=\"Another\"\n",
    "\n",
    "#text = \"\"\"I was born in the Great Depression to a carpenter father and a home keeping mother. Our family had a large garden, raised two pigs for their meat, and had a cow for her milk. When Dad lost his work, we started a laundry in our home. We hired up to seven girls at one time. Dad also rented 12 acres of nearby land to earn a few extra dollars. I was lucky to have good teachers to help in my learning. Following graduation, I spent two years in the army, allowing me to see Europe and build on my experiences. Then I returned home, got a job in the local bank and married my sweetheart. I was on my way to a normal lifestyle, but I was called back into the army when they built the Berlin Wall. That turned out to be a blessing too. As I hadn't worked long enough at the bank to begin health insurance, our oldest son, Ken, was born in an army hospital, which cost only $8.25. Following a year at Fort Chaffee, Arkansas, I returned home to work at the bank again. I worked my way up through the ranks to become Chairman, President, and the CEO of the bank before retiring in 1997. I was blessed to have had good employers and employees. I've been blessed to have a nice wife and wonderful children and grandchildren. I'm truly thankful to all I've met along the way. Today, I look <mask> on those grand experiences and see they were my possibilities to grow. I found many opportunities. They taught me to save for a rainy day. They taught me to help those less fortunate than me. They taught me to put others first, if I wanted to be successful in life.\"\"\"\n",
    "#answer = 'back'\n",
    "\n",
    "text = \"\"\"A lady once wrote a long story. She <mask> it to a famous editor. After a few weeks the editor returned the story to her. The lady was angry. She wrote back to the editor: \"Dear Sir, Yesterday you sent back a story of mine. How do you know that the story is not good? You did not read it. Before I sent you the story, I pasted together pages 18, 19 and 20. This was a test to see whether you would read the story. When the story came back yesterday, the pages were still pasted together. Is this the way you read all the stories that are sent to you?\" The editor wrote back: \"Dear Madam, when I have an egg for breakfast, I don't have to eat the whole egg in order to discover that it is bad.\" \"\"\"\n",
    "answer = 'sent'\n",
    "\n",
    "text=\"\"\"My  position  was to pass out water to the runners. I remember being so excited to see all the different kinds of  runners  who passed by and   grabbed  a cup of water. The next year I signed up for the race  and gave it a  <mask>  .\\nThe first 10,000 m race was quite an experience. I jogged, I walked, I jogged and I walked. At times, I didn't  know  if I could finish.\\nAt one point near the end, a 70-year-old man ran past me very  fast  , and I felt embarrassed that I was 50 years younger than him and I couldn't even keep up with him.\"\"\"\n",
    "answer = 'shot'\n",
    "\n",
    "#text = \"\"\"I was born in the Great Depression to a carpenter father and a home keeping mother. Our family had a large garden, raised two pigs for their meat, and had a cow for her milk. When Dad lost his work, we started a laundry in our home. We hired up to seven girls at one time. Dad also rented 12 acres of nearby land to earn a few extra dollars. I was lucky to have good teachers to help in my learning. Following graduation, I spent two years in the army, allowing me to see Europe and build on my experiences. Then I returned home, got a job in the local bank and married my sweetheart. I was on my way to a normal lifestyle, but I was called back into the army when they built the Berlin Wall. That turned out to be a blessing too. As I hadn't worked long enough at the bank to begin health insurance, our oldest son, Ken, was born in an army hospital, which cost only $8.25. Following a year at Fort Chaffee, Arkansas, I returned home to work at the bank again. I worked my way up through the ranks to become Chairman, President, and the CEO of the bank before retiring in 1997. I was blessed to have had good employers and employees. I've been blessed to have a nice wife and wonderful children and <mask>. I'm truly thankful to all I've met along the way. Today, I look back on those grand experiences and see they were my possibilities to grow. I found many opportunities. They taught me to save for a rainy day. They taught me to help those less fortunate than me. They taught me to put others first, if I wanted to be successful in life.\"\"\"\n",
    "#answer = 'grandchildren'\n",
    "\n",
    "MAX_SUBWORDS=1\n",
    "DISTRACTOR_POOL = 32\n",
    "DEBUG_OUTPUT=True\n",
    "CONTEXTUAL_EMBEDDING_LAYERS = [12]\n",
    "MIN_SENT_WORDS = 7\n",
    "ALPHA = 0.3\n",
    "\n",
    "sent_probs, para_probs, embs, emb_ans, distractors = candidates(text, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test annealing\n",
    "\n",
    "SIM_ANNEAL_EMB_WEIGHT = 0.25#4 # how much to weigh embedding distance vs. word likelihoods (increasing this value decreases plausibility while increasing diversity)\n",
    "CORRECTNESS_SENT = 1 # how much to weight probability of tokens in sentence-context model\n",
    "PLAUSIBILITY_DISTR_TO_ANS = 1#.25\n",
    "BETA=1\n",
    "indices = anneal(sent_probs, para_probs, embs, emb_ans, distractors, 3, answer)\n",
    "[distractors[x] for x in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_emb((T.as_tensor(sent_probs).log()-ALPHA*T.as_tensor(para_probs).log()).cpu().numpy(), embs, emb_ans, distractors, answer, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_ANNEAL_EMB_WEIGHT = 0.5#4 # how much to weigh embedding distance vs. word likelihoods (increasing this value decreases plausibility while increasing diversity)\n",
    "CORRECTNESS_SENT = 1 # how much to weight probability of tokens in sentence-context model\n",
    "PLAUSIBILITY_DISTR_TO_ANS = 1#.25\n",
    "MAX_SUBWORDS = 1\n",
    "DEBUG_OUTPUT = True\n",
    "\n",
    "text = \"\"\"I was born in the Great Depression to a carpenter father and a home keeping mother. Our family had a large garden, raised two pigs for their meat, and had a cow for her milk. When Dad lost his work, we started a laundry in our home. We hired up to seven girls at one time. Dad also rented 12 acres of nearby land to earn a few extra dollars. I was lucky to have good teachers to help in my learning. Following graduation, I spent two years in the army, allowing me to see Europe and build on my experiences. Then I returned home, got a job in the local bank and married my sweetheart. I was on my way to a normal lifestyle, but I was called back into the army when they built the Berlin Wall. That turned out to be a blessing too. As I hadn't worked long enough at the bank to begin health insurance, our oldest son, Ken, was born in an army hospital, which cost only $8.25. Following a year at Fort Chaffee, Arkansas, I returned home to work at the bank again. I worked my way up through the ranks to become Chairman, President, and the CEO of the bank before retiring in 1997. I was blessed to have had good employers and employees. I've been blessed to have a nice wife and wonderful children and grandchildren. I'm truly thankful to all I've met along the way. Today, I look back on those grand experiences and see they were my possibilities to grow. I found many opportunities. They taught me to save for a rainy day. They taught me to help those less fortunate than me. They taught me to put others first, if I wanted to be successful in life.\"\"\"\n",
    "choose_and_blank(text, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "ans2idx = {'A':0, 'B':1, 'C':2, 'D':3}\n",
    "idx2ans = {0:'A', 1:'B', 2:'C', 3:'D'}\n",
    "DEBUG_OUTPUT = False\n",
    "DISTRACTOR_POOL=128\n",
    "MAX_SUBWORDS = 1\n",
    "\n",
    "files = {\n",
    "    'high1':[\n",
    "        'high1-3734.cdgp-m.json',\n",
    "        'high2-3680.cdgp-m.json',\n",
    "    ],\n",
    "    'high2':[\n",
    "        'high3-4041.cdgp-m.json',\n",
    "        'high4-3780.cdgp-m.json'\n",
    "    ],\n",
    "    'high3':[\n",
    "        'high5-3946.cdgp-m.json',\n",
    "        'high6-4009.cdgp-m.json'\n",
    "    ],\n",
    "    'middle1':[\n",
    "        'middle1-2914.cdgp-m.json',\n",
    "        'middle2-2894.cdgp-m.json',\n",
    "        'middle3-2991.cdgp-m.json',\n",
    "        'middle4-2829.cdgp-m.json'\n",
    "    ],\n",
    "    'middle2':[\n",
    "        'middle5-3010.cdgp-m.json',\n",
    "        'middle6-2874.cdgp-m.json',\n",
    "        'middle7-2699.cdgp-m.json',\n",
    "        'middle8-2727.cdgp-m.json'\n",
    "    ],\n",
    "    'middle3':[\n",
    "        'middle9-2989.cdgp-m.json',\n",
    "        'middle10-2865.cdgp-m.json',\n",
    "        'middle11-2915.cdgp-m.json',\n",
    "        'middle12-2958.cdgp-m.json'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"text2text-generation\", model='Text2Text/text2text-t5-base/checkpoint-72000', device=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def t5multi(text, answer):\n",
    "    ds = generator(text.replace('<mask>', '_')+' [SEP] '+answer)[0]['generated_text'].split()\n",
    "    if len(ds) < 3:\n",
    "        ds *= 2\n",
    "    shuffle(ds)\n",
    "    return ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"My  _  was to pass out water to the runners. I remember being so excited to see all the different kinds of  runners  who passed by and   grabbed  a cup of water. The next year I signed up for the race  and gave it a  <mask>  .\\nThe first 10,000 m race was quite an experience. I jogged, I walked, I jogged and I walked. At times, I didn't  know  if I could finish.\\nAt one point near the end, a 70-year-old man ran past me very  fast  , and I felt embarrassed that I was 50 years younger than him and I couldn't even keep up with him.\"\"\"\n",
    "answer = 'shot'\n",
    "\n",
    "t5multi(text, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add nCloze and t5-Multi distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DISTRACTOR_POOL = 32\n",
    "ALPHA = 0.3\n",
    "BETA = 0.3\n",
    "MIN_SENT_WORDS = 7\n",
    "MIN_DIST = 7\n",
    "\n",
    "for block, fs in files.items():\n",
    "    for file in fs:\n",
    "        fn = 'cloth-test/%s/%s'%(block,file)\n",
    "        base = '.'.join(fn.split('.')[:-2])\n",
    "        print(base)\n",
    "        f = open(fn)\n",
    "        d = json.load(f)\n",
    "\n",
    "        t = d['article']\n",
    "        for i, a in enumerate(d['answers']):\n",
    "            answer = d['options'][i][ans2idx[a]]\n",
    "            t = t.replace('_', answer, 1)\n",
    "        newtext, dists, answers = choose_and_blank(t, len(d['answers']))\n",
    "        d['article-ncloze'] = newtext\n",
    "        d['options-ncloze'] = dists\n",
    "        d['answers-ncloze'] = answers\n",
    "        print(newtext)\n",
    "        \n",
    "        dists = []\n",
    "        distsT5 = []\n",
    "        answers = []\n",
    "        answersT5 = []\n",
    "        for i, a in enumerate(d['answers-cdgp-m']):\n",
    "            t = d['article-cdgp-m']\n",
    "            for j, o in enumerate(d['options-cdgp-m']):\n",
    "                if j == i:\n",
    "                    t = t.replace('_', '<mask>', 1)\n",
    "                else:\n",
    "                    answer = o[ans2idx[d['answers-cdgp-m'][j]]]\n",
    "                    t = t.replace('_', answer, 1)\n",
    "            answer = d['options-cdgp-m'][i][ans2idx[a]]\n",
    "            \n",
    "            ds = create_distractors(t, answer)\n",
    "            # insert the ansert randomly\n",
    "            idx = random.randint(0,3)\n",
    "            ds.insert(idx, answer)\n",
    "            dists.append(ds)\n",
    "            answers.append(idx2ans[idx])\n",
    "\n",
    "            ds = t5multi(t, answer)\n",
    "            # insert the ansert randomly\n",
    "            idx = random.randint(0,3)\n",
    "            ds.insert(idx, answer)\n",
    "            distsT5.append(ds)\n",
    "            answersT5.append(idx2ans[idx])\n",
    "            \n",
    "            #print(dists)\n",
    "            #print(answers)\n",
    "        d['options-ncloze-m'] = dists\n",
    "        d['answers-ncloze-m'] = answers\n",
    "        d['options-t5multi-m'] = distsT5\n",
    "        d['answers-t5multi-m'] = answersT5\n",
    "\n",
    "        #words = t.split()\n",
    "        #if len(words) > 256:\n",
    "        #    words = words[:256]\n",
    "        #t = ' '.join(words)\n",
    "        #print(t)\n",
    "        o = open(base + '.all.json', 'w')\n",
    "        o.write(json.dumps(d))\n",
    "        o.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEBUG_OUTPUT = False\n",
    "DISTRACTOR_POOL=32\n",
    "MAX_SUBWORDS = 1\n",
    "\n",
    "for a in [0.3]:\n",
    "    for deltab in [0.3]:\n",
    "        print(\"a: %f, b: %f\"%(a, b))\n",
    "\n",
    "        ALPHA = a\n",
    "        BETA = b\n",
    "        \n",
    "        for block, fs in files.items():\n",
    "            for file in fs:\n",
    "                fn = 'cloth-test/%s/%s'%(block,file)\n",
    "                base = '.'.join(fn.split('.')[:-2])\n",
    "                print(base)\n",
    "                f = open(base+'.ncloze.json')\n",
    "                d = json.load(f)\n",
    "                f.close()\n",
    "\n",
    "                t = d['article']\n",
    "                for i, a in enumerate(d['answers']):\n",
    "                    answer = d['options'][i][ans2idx[a]]\n",
    "                    t = t.replace('_', answer, 1)\n",
    "                words = t.split()\n",
    "                newtext, dists, answers = choose_and_blank(t, len(d['answers']))\n",
    "                d['article-ncloze-c'] = newtext\n",
    "                d['options-ncloze-c'] = dists\n",
    "                d['answers-ncloze-c'] = answers\n",
    "\n",
    "                o = open(base + '.ncloze-c.p%f.d%f.json'%(p,delta), 'w')\n",
    "                o.write(json.dumps(d))\n",
    "                o.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(d):\n",
    "    pairs = []\n",
    "\n",
    "    for i, a in enumerate(d['answers']):\n",
    "        t = d['article']\n",
    "        for j, o in enumerate(d['options']):\n",
    "            if j == i:\n",
    "                t = t.replace('_', '<mask>', 1)\n",
    "            else:\n",
    "                answer = o[ans2idx[d['answers'][j]]]\n",
    "                t = t.replace('_', answer, 1)\n",
    "        ds = d['options'][i]\n",
    "        ai = ans2idx[a]\n",
    "        answer = ds[ai]\n",
    "        ds = ds[:ai]+ds[ai+1:]\n",
    "        pairs.append((t, ds, answer))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import json\n",
    "#import csv\n",
    "#from random import random, shuffle\n",
    "\n",
    "ans2idx = {'A':0, 'B':1, 'C':2, 'D':3}\n",
    "idx2ans = {0:'A', 1:'B', 2:'C', 3:'D'}\n",
    "\n",
    "data = []\n",
    "for levl in ['middle', 'high']:\n",
    "    prfx = './CLOTH/valid/%s/'%(levl)\n",
    "    for file in os.listdir(prfx):\n",
    "        f = open(prfx + file)\n",
    "        d = json.load(f)\n",
    "        if '(1)' in d['article']:\n",
    "            print(\"Skipping %s...\"%file)\n",
    "        else:\n",
    "            data.extend(prepare(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_t = sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = []\n",
    "embs_val = []\n",
    "i = 0\n",
    "for pair in data[::10]:\n",
    "    i += 1\n",
    "    if not i % 1000:\n",
    "        print(i)\n",
    "    (text, ds, answer) = pair\n",
    "    doc = nlp(text)\n",
    "    #sents = [sentence.text for sentence in doc.sentences]\n",
    "    sents = nltk_sent_toker.tokenize(text)\n",
    "    msk_snt_idx = [i for i in range(len(sents)) if '<mask>' in sents[i]][0]\n",
    "    just_masked_sentence = sents[msk_snt_idx]\n",
    "    \n",
    "    prv_snts = sents[:msk_snt_idx]\n",
    "    nxt_snts = sents[msk_snt_idx+1:]\n",
    "    \n",
    "    if len(just_masked_sentence.split(' ')) < MIN_SENT_WORDS and len(prv_snts):\n",
    "        just_masked_sentence = ' '.join([prv_snts.pop(), just_masked_sentence])\n",
    "    \n",
    "    while len(just_masked_sentence.split(' ')) < MIN_SENT_WORDS and (len(prv_snts) or len(nxt_snts)):\n",
    "        if T.rand(1) < 0.5 and len(prv_snts):\n",
    "            just_masked_sentence = ' '.join([prv_snts.pop(), just_masked_sentence])\n",
    "        elif len(nxt_snts):\n",
    "            just_masked_sentence = ' '.join([just_masked_sentence, nxt_snts.pop(0)])\n",
    "    \n",
    "    #tiled = just_masked_sentence\n",
    "    #while len(tiled) < len(text):\n",
    "    #    tiled += ' ' + just_masked_sentence\n",
    "    #just_masked_sentence = tiled\n",
    "    \n",
    "    #print(just_masked_sentence)\n",
    "    toks_para = toker.encode(text)\n",
    "    toks_sent = toker.encode(just_masked_sentence)\n",
    "    sm_para = get_softmax_logits(toks_para, n_masks = 1, sub_ids = [])\n",
    "    sm_sent = get_softmax_logits(toks_sent, n_masks = 1, sub_ids = [])\n",
    "    sms.append((sm_para, sm_sent))\n",
    "    \n",
    "    es = []\n",
    "    for dist in ds:\n",
    "        #print(dist, toker.encode(dist)[1:-1])\n",
    "        es.append(get_emb(toks_para, toker.encode(dist)[1:-1]))\n",
    "    es.append(get_emb(toks_para, toker.encode(answer)[1:-1]))\n",
    "    embs_val.append(es)\n",
    "\n",
    "print(sms[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for i in range(len(embs_val)):\n",
    "    for j in range(len(embs_val[i])):\n",
    "        for k in range(j):\n",
    "            tot += -cos(embs_val[i][j], embs_val[i][k])\n",
    "\n",
    "tot /= len(embs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ALPHA = 0.3\n",
    "DISTRACTOR_POOL = 32\n",
    "DEBUG_OUTPUT=False\n",
    "bs = [.1, 0.3, 1, 3, 10]\n",
    "totals = {}\n",
    "correct = {}\n",
    "for b in bs:\n",
    "    totals[b] = 0\n",
    "    correct[b] = 0\n",
    "\n",
    "for datum in data[::10]:\n",
    "    text, answer = datum[0], datum[2]\n",
    "    sent_probs, para_probs, embs, emb_ans, distractors = candidates(text, answer)\n",
    "\n",
    "    embs += [emb_ans]\n",
    "    #print(datum)\n",
    "    for b in bs:\n",
    "        BETA = b\n",
    "        \n",
    "        indices = anneal(sent_probs, para_probs, embs, emb_ans, distractors, 3, answer)\n",
    "        indices += [len(embs)-1]\n",
    "        for j in range(len(indices)):\n",
    "            for k in range(j):\n",
    "                #print(j,k)\n",
    "                totals[b] += - cos(embs[indices[j]], embs[indices[k]])\n",
    "        #print(b, tot[b])\n",
    "        dists = [distractors[x] for x in indices[:-1]]\n",
    "        #print(dists)\n",
    "        for dist in dists:\n",
    "            if dist in datum[1]:\n",
    "                correct[b] += 1\n",
    "\n",
    "for b in bs:\n",
    "    print(b, totals[b]/10)\n",
    "    \n",
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for datum in data[::100]:\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=1e-10\n",
    "\n",
    "for a in [0.1, 0.3, 1, 3, 10]:\n",
    "    mrr = 0\n",
    "    for i in range(len(sms)):\n",
    "        (sm_para, sm_sent) = sms[i]\n",
    "        score = (sm_sent[0]*word_mask+e).log() - a * (sm_para[0]*word_mask+e).log()\n",
    "#        print(T.topk(score, 3).indices)\n",
    "        #print([sorted_toker_vocab_dict[x] for x in list(T.topk(score, 10).indices)])\n",
    "        #print(score.shape)\n",
    "        order = T.argsort(score, dim=0).squeeze()+1\n",
    "        #print(order.squeeze().shape)\n",
    "        for dist in data[i][1]:\n",
    "            tid = toker.encode(dist)[1]\n",
    "            mrr += 1./order[tid]\n",
    "        #break\n",
    "    mrr /= 3*len(sms)\n",
    "    print(a, mrr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distractor scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(text, answer, dists):\n",
    "    embs = []\n",
    "    \n",
    "    phis = []\n",
    "    Phis = []\n",
    "\n",
    "    doc = nlp(text)\n",
    "    #sents = [sentence.text for sentence in doc.sentences]\n",
    "    sents = nltk_sent_toker.tokenize(text)\n",
    "    msk_snt_idx = [i for i in range(len(sents)) if '<mask>' in sents[i]][0]\n",
    "    just_masked_sentence = sents[msk_snt_idx]\n",
    "\n",
    "    prv_snts = sents[:msk_snt_idx]\n",
    "    nxt_snts = sents[msk_snt_idx+1:]\n",
    "\n",
    "    if len(just_masked_sentence.split(' ')) < MIN_SENT_WORDS and len(prv_snts):\n",
    "        just_masked_sentence = ' '.join([prv_snts.pop(), just_masked_sentence])\n",
    "\n",
    "    while len(just_masked_sentence.split(' ')) < MIN_SENT_WORDS and (len(prv_snts) or len(nxt_snts)):\n",
    "        if T.rand(1) < 0.5 and len(prv_snts):\n",
    "            just_masked_sentence = ' '.join([prv_snts.pop(), just_masked_sentence])\n",
    "        elif len(nxt_snts):\n",
    "            just_masked_sentence = ' '.join([just_masked_sentence, nxt_snts.pop(0)])\n",
    "\n",
    "    #tiled = just_masked_sentence\n",
    "    #while len(tiled) < len(text):\n",
    "    #    tiled += ' ' + just_masked_sentence\n",
    "    #just_masked_sentence = tiled\n",
    "\n",
    "    #print(just_masked_sentence)\n",
    "    toks_para = toker.encode(text)\n",
    "    toks_sent = toker.encode(just_masked_sentence)\n",
    "    sm_para = get_softmax_logits(toks_para, n_masks = 1, sub_ids = [])\n",
    "    sm_sent = get_softmax_logits(toks_sent, n_masks = 1, sub_ids = [])\n",
    "\n",
    "    for dist in dists:\n",
    "        tok_dist = toker.encode(dist)[1:-1]\n",
    "        phis.append(float(sm_sent[0][tok_dist].mean().log()))\n",
    "        Phis.append(float(-sm_para[0][tok_dist].mean().log()))\n",
    "        #print(dist, toker.encode(dist)[1:-1])\n",
    "        embs.append(get_emb(toks_para, tok_dist))\n",
    "    \n",
    "    embs.append(get_emb(toks_para, toker.encode(answer)[1:-1]))\n",
    "    delta = 0.\n",
    "    for i in range(len(embs)):\n",
    "        for j in range(i):\n",
    "            delta += 1. - float(cos(embs[i], embs[j]))\n",
    "\n",
    "    return phis, Phis, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([0, 1,2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnd2art = {\n",
    "    'cloth':'article',\n",
    "    'cdgp-m':'article-cdgp-m',\n",
    "    't5multi-m':'article-cdgp-m',\n",
    "    'ncloze-m':'article-cdgp-m',\n",
    "    'ncloze':'article-ncloze'\n",
    "}\n",
    "\n",
    "f = open('compiled.json')\n",
    "d = json.load(f)\n",
    "f.close()\n",
    "\n",
    "od = open('scores-dist.tsv', 'w')\n",
    "oq = open('scores-ques.tsv', 'w')\n",
    "\n",
    "labl=['A', 'B', 'C', 'D']\n",
    "\n",
    "for pasg in d:\n",
    "    print(pasg)\n",
    "    for cond in cnd2art:\n",
    "        print('  ' + cond)\n",
    "        key = ('' if cond == 'cloth' else '-'+cond)\n",
    "        anss = d[pasg]['answers' + key]\n",
    "        opts = d[pasg]['options' + key]\n",
    "        arti = d[pasg][cnd2art[cond]]\n",
    "        for i, ans in enumerate(anss):\n",
    "            qkey = '%s-%s-%d'%(pasg, cond, i)\n",
    "            t = arti\n",
    "            aidx = ans2idx[ans]\n",
    "            dists = opts[i][:aidx]+opts[i][aidx+1:]\n",
    "            for j, opt in enumerate(opts):\n",
    "                if j == i:\n",
    "                    t = t.replace('_', '<mask>', 1)\n",
    "                else:\n",
    "                    t = t.replace('_', opt[ans2idx[anss[j]]], 1)\n",
    "            sp = t.split('<br/><br/>')\n",
    "            t = sp[0] if '<mask>' in sp[0] else sp[1]\n",
    "            phis, Phis, delta = score(t, opts[i][aidx], dists)\n",
    "            labls = labl[:aidx]+labl[aidx+1:]\n",
    "            for j in range(len(phis)):\n",
    "                od.write('\\t'.join([qkey+'-'+labls[j], str(phis[j]), str(Phis[j])])+'\\n')\n",
    "            oq.write('\\t'.join([qkey, str(np.sum(phis)), str(np.sum(Phis)), str(delta)])+'\\n')\n",
    "#        break\n",
    "#    break\n",
    "od.close()\n",
    "oq.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "CSG_MODEL = \"AndyChiang/cdgp-csg-bert-cloth\"\n",
    "DS_MODEL = \"./cdgp-ds-fasttext.bin\"\n",
    "TOP_K = 3\n",
    "STOP_WORDS = [\"[MASK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\"]\n",
    "WEIGHT = {\"s0\": 0.6, \"s1\": 0.15, \"s2\": 0.15, \"s3\": 0.1}\n",
    "# WEIGHT = {\"s0\": 0.25, \"s1\": 0.25, \"s2\": 0.25, \"s3\": 0.25}\n",
    "QUESTION_LIMIT = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load CSG model\n",
    "print(f\"Load CSG model at {CSG_MODEL}...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(CSG_MODEL)\n",
    "csg_model = BertForMaskedLM.from_pretrained(CSG_MODEL)\n",
    "\n",
    "# Create a unmasker\n",
    "unmasker = pipeline('fill-mask', tokenizer=tokenizer, model=csg_model, top_k=TOP_K)\n",
    "\n",
    "# Load DS model\n",
    "print(f\"Load DS model at {DS_MODEL}...\")\n",
    "ds_model = fasttext.load_model(DS_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate distractors of one question\n",
    "def generate_dis(unmasker, ds_model, sent, answer):\n",
    "    # Answer relating\n",
    "    target_sent = sent + \" [SEP] \" + answer\n",
    "\n",
    "    # Generate Candidate Set\n",
    "    cs = list()\n",
    "    for cand in unmasker(target_sent):\n",
    "        word = cand[\"token_str\"].replace(\" \", \"\")\n",
    "        if len(word) > 0:  # Skip empty\n",
    "            cs.append({\"word\": word, \"s0\": cand[\"score\"], \"s1\": 0.0, \"s2\": 0.0, \"s3\": 0.0})\n",
    "\n",
    "    # Confidence Score s0\n",
    "    s0s = [c[\"s0\"] for c in cs]\n",
    "    new_s0s = min_max_y(s0s)\n",
    "\n",
    "    for i, c in enumerate(cs):\n",
    "        c[\"s0\"] = new_s0s[i]\n",
    "\n",
    "    # Word Embedding Similarity s1\n",
    "    answer_vector = ds_model.get_word_vector(answer)\n",
    "    word_similarities = list()\n",
    "    for c in cs:\n",
    "        c_vector = ds_model.get_word_vector(c[\"word\"])\n",
    "        word_similarity = similarity(answer_vector, c_vector)   # Cosine similarity between A and Di\n",
    "        word_similarities.append(word_similarity)\n",
    "\n",
    "    new_similarities = min_max_y(word_similarities)\n",
    "\n",
    "    for i, c in enumerate(cs):\n",
    "        c[\"s1\"] = 1-new_similarities[i]\n",
    "\n",
    "    # Contextual-Sentence Embedding Similarity s2\n",
    "    correct_sent = sent.replace('[MASK]', answer)\n",
    "    correct_sent_vector = ds_model.get_sentence_vector(correct_sent)\n",
    "\n",
    "    cand_sents = list()\n",
    "    for c in cs:\n",
    "        cand_sents.append(sent.replace('[MASK]', c[\"word\"]))\n",
    "\n",
    "    sent_similarities = list()\n",
    "    for cand_sent in cand_sents:\n",
    "        cand_sent_vector = ds_model.get_sentence_vector(cand_sent)\n",
    "        sent_similarity = similarity(correct_sent_vector, cand_sent_vector) # Cosine similarity between S(A) and S(Di)\n",
    "        sent_similarities.append(sent_similarity)\n",
    "\n",
    "    new_similarities = min_max_y(sent_similarities)\n",
    "    for i, c in enumerate(cs):\n",
    "        c[\"s2\"] = 1-new_similarities[i]\n",
    "\n",
    "    # POS match score s3\n",
    "    origin_token = word_tokenize(sent)\n",
    "    origin_token.remove(\"[\")\n",
    "    origin_token.remove(\"]\")\n",
    "\n",
    "    mask_index = origin_token.index(\"MASK\")\n",
    "    \n",
    "    correct_token = word_tokenize(correct_sent)\n",
    "    correct_pos = nltk.pos_tag(correct_token)\n",
    "    answer_pos = correct_pos[mask_index]    # POS of A\n",
    "\n",
    "    for i, c in enumerate(cs):\n",
    "        cand_sent_token = word_tokenize(cand_sents[i])\n",
    "        cand_sent_pos = nltk.pos_tag(cand_sent_token)\n",
    "        cand_pos = cand_sent_pos[mask_index]    # POS of Di\n",
    "\n",
    "        if cand_pos[1] == answer_pos[1]:\n",
    "            c[\"s3\"] = 1.0\n",
    "        else:\n",
    "            c[\"s3\"] = 0.0\n",
    "\n",
    "    # Weighted final score\n",
    "    cs_rank = list()\n",
    "    for c in cs:\n",
    "        fs = WEIGHT[\"s0\"]*c[\"s0\"] + WEIGHT[\"s1\"]*c[\"s1\"] + WEIGHT[\"s2\"]*c[\"s2\"] + WEIGHT[\"s3\"]*c[\"s3\"]\n",
    "        cs_rank.append((c[\"word\"], fs))\n",
    "\n",
    "    # Rank by final score\n",
    "    cs_rank.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Top K\n",
    "    result = [d[0] for d in cs_rank[:TOP_K]]\n",
    "\n",
    "    return result\n",
    "\n",
    "def CDGP(text, answer):\n",
    "    return generate_dis(unmasker, ds_model, text, answer)\n",
    "\n",
    "# Cosine similarity\n",
    "def similarity(v1, v2):\n",
    "    n1 = np.linalg.norm(v1)\n",
    "    n2 = np.linalg.norm(v2)\n",
    "    if n1 == 0 or n2 == 0:  # Denominator can not be zero\n",
    "        return 1\n",
    "    else:\n",
    "        return np.dot(v1, v2) / (n1 * n2)\n",
    "\n",
    "# Minâmax normalization\n",
    "def min_max_y(raw_data):\n",
    "    min_max_data = []\n",
    "\n",
    "    # Minâmax normalization\n",
    "    for d in raw_data:\n",
    "        min_max_data.append((d - min(raw_data)) / (max(raw_data) - min(raw_data)))\n",
    "\n",
    "    return min_max_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Perhaps because I was city kid, my exposure to wildlife was limited. That changed when I moved to the wooded hills of Oregon [MASK] years later. For the first time, I met animal communities. One evening, a nursing raccoon with four kids appeared. She extended her tiny paw as if asking for some food. I was attracted by their cuteness, so I instantly put out a serving of fresh cat food and water. She returned the next evening. And the next. All was well until the wildlife began behaving wildly. The raccoons started crying noisily. They could be heard throughout the entire valley. A few days later, our homeowners association newsletter arrived in the mail. Among the usual announcements of garage sales came a gentle reminder that feeding the wildlife was not a(n) suitable thing to do. My face became red with embarrassment as I read the letter. I'd been found out! I was now identified as the trouble maker! I went downstairs to discuss the matter with my husband. \"I'm not surprised that the association has come up with a policy about it. They must have gotten complaints,\" he said. \"OK, I'm going to stop feeding the animals,\" I said. Although I told myself that the wildlife around me would survive without cat food, I felt guilty. Late that night, I walked slowly into the kitchen for a snack. Then a scene outside attracted my attention: There, on the hillside, was my neighbor. She was feeding two deer in the cold.\"\"\"\n",
    "text = \"They would never [MASK] to see their family again.\"\n",
    "answer = 'want'\n",
    "ds = CDGP(text, answer)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to Choose and Preprocess Chosen Target Term(s) from a Full Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_spaced_multiple_questions_with_choices_from_sentences(text, index_of_starting_word=20, number_of_distractors=4, interval_between_words=INTERVAL_BETWEEN_WORDS, max_number_of_questions = MAX_QUESTIONS_BY_N):\n",
    "#\n",
    "blanked_texts = {}\n",
    "#     number_of_questions = 0\n",
    "#     text = text.strip()\n",
    "    \n",
    "#     # Create list of all words with punctuation\n",
    "#     all_words = text.split(' ')\n",
    "#     full_masked_words = all_words.copy()\n",
    "#     for j in range(index_of_starting_word, len(all_words), interval_between_words):\n",
    "        \n",
    "#         # Create new list of full words and replace ONLY ONE target term in it\n",
    "#         words = all_words.copy()\n",
    "        \n",
    "#         # Replace target term with a mask\n",
    "#         target_term = words[j]\n",
    "#         # Check for punctuation and then replace target term (without punctuation) with a mask\n",
    "#         if target_term.strip(punctuation) != target_term:\n",
    "        \n",
    "#             # Only mask the part of the word not connected to punctuation\n",
    "#             split_list = re.split(\"(\" + str(target_term.strip(punctuation)) + \")\", target_term)\n",
    "#             for i in range(len(split_list)):\n",
    "#                 if split_list[i] == target_term.strip(punctuation):\n",
    "#                     target_term = split_list[i]\n",
    "#                     split_list[i] = toker.mask_token\n",
    "#                     break\n",
    "#             words[j] = ''.join(split_list)\n",
    "#             # Replace and accumulate ALL target terms in full_masked_words\n",
    "#             full_masked_words[j] = ''.join(split_list)\n",
    "#         else:\n",
    "#             words[j] = toker.mask_token\n",
    "#             # Replace and accumulate ALL target terms in full_masked_words\n",
    "#             full_masked_words[j] = toker.mask_token\n",
    "            \n",
    "#         number_of_questions += 1\n",
    "        \n",
    "        \n",
    "#         blanked_text = ' '.join(words)\n",
    "#         blanked_text = blanked_text + \" \"\n",
    "#         blanked_text = blanked_text.replace('nothingtoseehere. ', '')\n",
    "#         blanked_text = blanked_text.strip()\n",
    "#         list_of_choices = create_distractors(blanked_text, target_term, number_of_distractors)\n",
    "#         print('Distractors chosen after annealing:', list_of_choices)\n",
    "            \n",
    "#         # Insert target term (correct answer) in front\n",
    "#         list_of_choices.insert(0, target_term)\n",
    "        \n",
    "#         blanked_texts[number_of_questions] = list_of_choices\n",
    "#         if len(blanked_texts) >= max_number_of_questions:\n",
    "#             break\n",
    "        \n",
    "#     full_masked_text = ' '.join(full_masked_words)\n",
    "#     return pd.Series([number_of_questions, blanked_texts, full_masked_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiple_questions_with_choices_from_sentences_by_distractors(row, number_of_distractors=3):\n",
    "    blanked_texts = {}\n",
    "    number_of_questions = 0\n",
    "    text = row['text'].strip()\n",
    "    all_masks_text = row['masked_text']\n",
    "    \n",
    "    # Create list of all words with punctuation\n",
    "    all_words = text.split(' ')\n",
    "    all_masked_words = all_masks_text.split(' ')\n",
    "    for word_index, word in enumerate(all_masked_words):\n",
    "        \n",
    "        # Only for masked words\n",
    "        if \"<mask>\" in word:\n",
    "        \n",
    "            # Create new list of full text and replace ONLY ONE target term in it\n",
    "            all_words_copy = all_words.copy()\n",
    "            # Replace target term with a mask\n",
    "            target_term = all_words_copy[word_index]\n",
    "            # Check for punctuation and then replace target term (without punctuation) with a mask\n",
    "            if target_term.strip(punctuation) != target_term:\n",
    "                # Only mask the part of the word not connected to punctuation\n",
    "                split_list = re.split(\"(\" + str(target_term.strip(punctuation)) + \")\", target_term)\n",
    "                for i in range(len(split_list)):\n",
    "                    if split_list[i] == target_term.strip(punctuation):\n",
    "                        target_term = split_list[i]\n",
    "                        split_list[i] = toker.mask_token\n",
    "                        break\n",
    "                all_words_copy[word_index] = ''.join(split_list)\n",
    "                \n",
    "            else:\n",
    "                all_words_copy[word_index] = toker.mask_token\n",
    "                \n",
    "            number_of_questions += 1\n",
    "            one_mask_text = ' '.join(all_words_copy)\n",
    "            one_mask_text = one_mask_text.strip()\n",
    "            list_of_choices = anneal(create_distractors(one_mask_text, target_term)+(number_of_distractors, target_term))\n",
    "            print('Distractors chosen after annealing:', list_of_choices)\n",
    "            \n",
    "            # Insert target term (correct answer) in front\n",
    "            list_of_choices.insert(0, target_term)\n",
    "            \n",
    "            blanked_texts[number_of_questions] = list_of_choices\n",
    "    return pd.Series([number_of_questions, blanked_texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Cloze Tasks with Adaptation Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe\n",
    "df = pd.read_csv(\"Cloth_Text_Distractors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe to hold the file to use for creating the Turker Task\n",
    "turker_input_df = df\n",
    "# Delete unnecessary columns\n",
    "turker_input_df = turker_input_df[[\"pmid\", \"text\", \"masked_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turker_input_df = turker_input_df.head(1)\n",
    "\n",
    "turker_input_df.loc[0, 'masked_text'] = \"\"\"\n",
    "The Healing Jim and his wife, Connie, were shocked by the loss of their four-month-old son--Joshua, whose life was taken by SIDS--sudden infant death syndrome. Thirty hours ago Jim drove to the baby-sitter's home to pick up Joshua. It was a routine trip, like the one he made five days every week. He arrived, and little Joshua could not be awakened from his nap. The next few hours were a time of life and death: the racing ambulance, swift-moving doctors and nurses....but 12 hours later, at children's Hospital, though the doctors had exhausted all  attempts, little Joshua was gone. Yes, they wanted all of Joshua's usable organs to be donated. That was not a difficult decision for Jim and Connie, a loving and giving couple. The next morning dawned and many things had to be arranged. Telephone calls and funeral plans. At one point Jim realized he needed a haircut. When Jim settled into the chair at the barber's, he began to reflect on the past hours, trying to make some sense of it all. Why had Joshua, their first-born, the child they had waited so long for, been taken so soon....he had barely begun his life....The question kept coming, and the pain in Jim's heart just enveloped him. While talking with the barber, Jim mentioned the organ donations, looking at his watch: \"They are transplanting one of his heart valves right now.\" The hairdresser stopped and stood motionless. Finally she spoke, but it was only a whisper. \"You're not going to believe this....but about an hour ago the customer sitting in this chair wanted me to hurry so she could get to Children's Hospital. She left here so full of joy....her prayers had been answered. Today her baby granddaughter is receiving a desperately needed transplant--a heart <mask>.\" Jim's healing began.\n",
    "\"\"\"\n",
    "\n",
    "# turker_input_df.loc[1, 'masked_text'] = \"\"\"\n",
    "# In 1977, a dead author of detective stories saved the life of a 19-month-old baby in a most unusual way. The author was Agatha Christie, one of the most successful writers of detective stories in the world. In June 1977, a baby girl became seriously ill in Qatar, near Saudi Arabia. Doctors were unable to <mask> the cause of her illness, so she was flown to London and admitted to Hammersmith Hospital, where specialist help was <mask>. She was then only half-conscious and on the \"Dangerously Ill\" list. A team of doctors hurried to examine the baby only to discover that they, too, were puzzled by the very unusual symptoms. While they were discussing the baby's case, a nurse asked to speak to them. \"Excuse me,\" said nurse Marsha Maitland, \"but I think the baby is suffering from thallium poisoning.\" \"What makes you think that?\" Dr. Brown asked. \"Thallium poisoning is extremely rare.\" \"A few days ago, I was reading a novel called by Agatha Christie,\" Nurse Maitland explained. \"In the book, somebody uses thallium poison, and all the symptoms are described. They are exactly the same as the baby's.\" \"You're very observant and you may be right,\" another doctor said. \"We'll carry out some tests and find out whether it's thallium or not.\" The tests showed that the baby had indeed been poisoned by thallium, a rare metal used in making optical glass. Once they knew the cause of illness, the doctors were able to give the correct treatment. The baby soon recovered and was sent back to Qatar. Inquiries showed that the poison might have come from an insecticide used in Qatar.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_choices = NUM_OF_CHOICES\n",
    "\n",
    "# Create columns with output of cloze_question_distractor_generator method\n",
    "turker_input_df[['number_of_questions', 'full_nth_info']] = turker_input_df.apply(lambda x: create_multiple_questions_with_choices_from_sentences_by_distractors(x), axis=1)\n",
    "\n",
    "# Create blanked version of masked_text texts\n",
    "turker_input_df['blanked_text'] = turker_input_df['masked_text'].apply(lambda x: x.replace(\"<mask>\", \"_\"))\n",
    "\n",
    "# Find new maximum of questions\n",
    "MAX_QUESTIONS_BY_N = turker_input_df['number_of_questions'].max()\n",
    "\n",
    "# Create new dataframe that is just lists of all the answer choices\n",
    "all_answer_choices_df = turker_input_df['full_nth_info'].apply(lambda x: pd.Series(x))\n",
    "\n",
    "# Create column names to replace old column names\n",
    "column_names_to_add = ['answer_choices_word' + str(i) for i in all_answer_choices_df.columns]\n",
    "keys_to_add = list(range(1, MAX_QUESTIONS_BY_N+1))\n",
    "\n",
    "# Rename dataframe\n",
    "renamed_all_answer_choices_df = all_answer_choices_df.rename(columns=dict(zip(keys_to_add, column_names_to_add)))\n",
    "\n",
    "#turker_input_df[list(renamed_all_answer_choices_df.columns)] = renamed_all_answer_choices_df\n",
    "for chosen_word_number in range(MAX_QUESTIONS_BY_N):\n",
    "    # Fill all empty values with lists\n",
    "    renamed_all_answer_choices_df['answer_choices_word' + str(chosen_word_number+1)] = renamed_all_answer_choices_df['answer_choices_word' + str(chosen_word_number+1)].apply(lambda d: d if isinstance(d, list) else [''] * NUM_OF_CHOICES)\n",
    "\n",
    "    # Add renamed column of answer choices to turker_input_df\n",
    "    turker_input_df['answer_choices_word' + str(chosen_word_number+1)] = renamed_all_answer_choices_df['answer_choices_word' + str(chosen_word_number+1)]\n",
    "\n",
    "    all_answers_df = pd.DataFrame(renamed_all_answer_choices_df['answer_choices_word' + str(chosen_word_number+1)].tolist(), index=renamed_all_answer_choices_df.index)\n",
    "    turker_input_df['correct_answer_word' + str(chosen_word_number+1)] = all_answers_df[0]\n",
    "    for i in range(number_of_choices):\n",
    "        turker_input_df['choice_' + str(i+1) + '_word' + str(chosen_word_number+1)] = all_answers_df[i]\n",
    "\n",
    "\n",
    "# Reset index\n",
    "turker_input_df = turker_input_df.reset_index(drop=True)\n",
    "\n",
    "for chosen_word_number in range(MAX_QUESTIONS_BY_N):\n",
    "    # Shuffle the answers for the first set of questions\n",
    "    columns_to_shuffle = [\"choice_\" + str(x+1) + \"_word\" + str(chosen_word_number+1) for x in range(number_of_choices)]\n",
    "    start_columms_indices_to_shuffle = turker_input_df.columns.get_loc(\"choice_1\" + '_word' + str(chosen_word_number+1))\n",
    "    for index, row in turker_input_df[columns_to_shuffle].iterrows():\n",
    "        turker_input_df.iloc[index, start_columms_indices_to_shuffle:(start_columms_indices_to_shuffle + len(columns_to_shuffle))] = row.sample(frac=1, random_state=random.randint(1, 500)).reset_index(drop=True)\n",
    "\n",
    "# Create new series that is just lists of all the texts\n",
    "masked_text_split_list_series = turker_input_df['masked_text'].apply(lambda x: x.split('<mask>'))\n",
    "\n",
    "# Create dataframe that is just the texts \n",
    "all_text_df = pd.DataFrame(masked_text_split_list_series.tolist(), index=masked_text_split_list_series.index)\n",
    "\n",
    "for chosen_word_number in range(MAX_QUESTIONS_BY_N):\n",
    "\n",
    "    turker_input_df['text_for_word' + str(chosen_word_number+1)] = all_text_df[chosen_word_number]\n",
    "    if chosen_word_number == (MAX_QUESTIONS_BY_N-1):\n",
    "        turker_input_df['text_for_wordfinal'] = all_text_df[chosen_word_number + 1]\n",
    "\n",
    "# Fill empty rows with empty spaces \n",
    "turker_input_df = turker_input_df.fillna('')\n",
    "\n",
    "turker_input_df.to_csv('Cloth_Text_nCloze_Distractors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
